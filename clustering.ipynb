{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd58e39-68ef-4c57-acae-306eec29dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "# Corpus Processing\n",
    "import re\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.preprocessing            import normalize\n",
    "\n",
    "# K-Means\n",
    "from sklearn import cluster\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "997c03c5-e86b-43f3-a495-60e687156c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('usc_seating_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73e2fbc8-bc1e-4da3-99bb-4878552ac260",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Abstract 1'] = data['Abstract 1'].fillna(' ')\n",
    "data['Abstract 2'] = data['Abstract 2'].fillna(' ')\n",
    "data['Abstract 3'] = data['Abstract 3'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bea1c850-eb4e-437f-a0ba-521dc09da811",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['added_abstracts'] = data['Abstract 1'] + ' ' + data['Abstract 2'] + ' ' + data['Abstract 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a8cd7f-a5e1-4df1-8459-184890a63760",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data[['First name', 'Last Name', 'School', 'Category', 'PhD', 'Provided Keywords', 'Random', 'Skip', 'added_abstracts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79ea85d0-8296-485d-a195-cf2ddfdcb474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>School</th>\n",
       "      <th>Category</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Provided Keywords</th>\n",
       "      <th>Random</th>\n",
       "      <th>Skip</th>\n",
       "      <th>added_abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kevin</td>\n",
       "      <td>Murphy</td>\n",
       "      <td>USC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Finance theory says that companies in declinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert</td>\n",
       "      <td>Dittmar</td>\n",
       "      <td>University of Michigan</td>\n",
       "      <td>Finance</td>\n",
       "      <td>No</td>\n",
       "      <td>Asset pricing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We conduct a systematic examination of the ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mitch</td>\n",
       "      <td>Warachka</td>\n",
       "      <td>Chapman University</td>\n",
       "      <td>Finance</td>\n",
       "      <td>No</td>\n",
       "      <td>Innovation\\nValue Creation\\nCulture</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A large literature reports that proximity infl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>William</td>\n",
       "      <td>Mullins</td>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>Finance</td>\n",
       "      <td>No</td>\n",
       "      <td>Finance, Corporate and Household finance, poli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We find evidence of selective exposure to conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stan</td>\n",
       "      <td>Markov</td>\n",
       "      <td>UT Dallas</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>No</td>\n",
       "      <td>social media, big data, market efficiency,\\</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We examine how increased competition stemming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Kristi</td>\n",
       "      <td>Rennekamp</td>\n",
       "      <td>Cornell University</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As firms increasingly use social media to prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Suzanne</td>\n",
       "      <td>Burzillo</td>\n",
       "      <td>USC</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recent years have witnessed growing interest i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Nick</td>\n",
       "      <td>Bloom</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>Economics</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We construct the World Uncertainty Index (WUI)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Anastassia</td>\n",
       "      <td>Fedyk</td>\n",
       "      <td>Berkeley Haas</td>\n",
       "      <td>Finance</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We study the use and economic impact of artifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Wenhao</td>\n",
       "      <td>Li</td>\n",
       "      <td>USC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>No</td>\n",
       "      <td>Financial Intermediation, Asset Pricing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bank-created money, shadow-bank money, and Tre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    First name  Last Name                  School     Category  PhD  \\\n",
       "0        Kevin     Murphy                     USC      Finance   No   \n",
       "1       Robert    Dittmar  University of Michigan      Finance   No   \n",
       "2        Mitch   Warachka      Chapman University      Finance   No   \n",
       "3      William    Mullins            UC San Diego      Finance   No   \n",
       "4         Stan     Markov               UT Dallas   Accounting   No   \n",
       "..         ...        ...                     ...          ...  ...   \n",
       "73      Kristi  Rennekamp      Cornell University  Accounting    No   \n",
       "74     Suzanne   Burzillo                     USC   Accounting  Yes   \n",
       "75        Nick      Bloom                Stanford    Economics   No   \n",
       "76  Anastassia      Fedyk           Berkeley Haas      Finance   No   \n",
       "77      Wenhao         Li                     USC      Finance   No   \n",
       "\n",
       "                                    Provided Keywords Random Skip  \\\n",
       "0                                                 NaN    NaN  NaN   \n",
       "1                                       Asset pricing    NaN  NaN   \n",
       "2                 Innovation\\nValue Creation\\nCulture    NaN  NaN   \n",
       "3   Finance, Corporate and Household finance, poli...    NaN  NaN   \n",
       "4         social media, big data, market efficiency,\\    NaN  NaN   \n",
       "..                                                ...    ...  ...   \n",
       "73                                                NaN    NaN  NaN   \n",
       "74                                                NaN    NaN  NaN   \n",
       "75                                                NaN    NaN  NaN   \n",
       "76                                                NaN    NaN  NaN   \n",
       "77            Financial Intermediation, Asset Pricing    NaN  NaN   \n",
       "\n",
       "                                      added_abstracts  \n",
       "0   Finance theory says that companies in declinin...  \n",
       "1   We conduct a systematic examination of the ret...  \n",
       "2   A large literature reports that proximity infl...  \n",
       "3   We find evidence of selective exposure to conf...  \n",
       "4   We examine how increased competition stemming ...  \n",
       "..                                                ...  \n",
       "73  As firms increasingly use social media to prov...  \n",
       "74  Recent years have witnessed growing interest i...  \n",
       "75  We construct the World Uncertainty Index (WUI)...  \n",
       "76  We study the use and economic impact of artifi...  \n",
       "77  Bank-created money, shadow-bank money, and Tre...  \n",
       "\n",
       "[78 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5a5b7da-d65b-4dac-9b3b-72a1efd608d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data_cleaned['added_abstracts'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1adfcae-82e8-468d-9afd-f3159abaee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes stopwords from a tokenized list\n",
    "def remove_stopwords(tokens, words):\n",
    "    return [token for token in tokens if token not in words]\n",
    "\n",
    "# apply stemming to a list of tokens\n",
    "def apply_stemming(tokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# find words <= 2 letters or >= 21 letters\n",
    "def find_two_letters(tokens):\n",
    "    two_letters = []\n",
    "    for token in tokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            two_letters.append(token)\n",
    "    return two_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a648bda0-0491-41ed-b0bf-f1d7d8eab860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):   \n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    param_stemmer = SnowballStemmer(language)\n",
    "    other_words = [line.rstrip('\\n') for line in open('stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = find_two_letters(listOfTokens)\n",
    "\n",
    "        listOfTokens = remove_stopwords(listOfTokens, stopwords)\n",
    "        listOfTokens = remove_stopwords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = apply_stemming(listOfTokens, param_stemmer)\n",
    "        listOfTokens = remove_stopwords(listOfTokens, other_words)\n",
    "\n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd15eab5-2949-4cf1-852f-4d308a188a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alekseyvalouev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alekseyvalouev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "678698de-5980-4f3c-a69c-ba0db2b7c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use trade-level data examin role activ manag fund ( amf ) earn news dissemin . find amf drawn particip disproportion earn announc ( ea ) includ bundl manageri guidanc . two piec news direct inconsist amf trade direct futur guidanc rather current earn . amf exhibit abil discern adapt trade bias bundl guidanc . amf trade ea general profit non-ea trade result revers guidanc bias extrem . overal find increas amf trade ea lead faster price adjust . collect find'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = 'english'\n",
    "corpus = processCorpus(corpus, language)\n",
    "corpus[18][0:460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975bc58-9668-4f14-9364-c2ccffd0b929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
